{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a245ddcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/01 16:51:47 WARN Utils: Your hostname, jl-hp resolves to a loopback address: 127.0.1.1; using 192.168.31.13 instead (on interface wlo1)\n",
      "22/05/01 16:51:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/jl/miniconda3/envs/6998/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/01 16:51:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import Row, SparkSession, SQLContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "app_name = \"prac\"\n",
    "\n",
    "conf = SparkConf().setAppName(app_name).setMaster(\"local[*]\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "spark = SparkSession.builder.appName(app_name).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68a74851",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.141162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "from operator import add\n",
    "\n",
    "partitions = 1\n",
    "n = 10000000 * partitions\n",
    "\n",
    "def f(_):\n",
    "    x = random() * 2 - 1\n",
    "    y = random() * 2 - 1\n",
    "    return 1 if x ** 2 + y ** 2 <= 1 else 0\n",
    "\n",
    "count = sc.parallelize(range(1, n + 1), partitions).map(f).reduce(add)\n",
    "print(\"Pi is roughly %f\" % (4.0 * count/n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9973ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, date\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "], schema='a long, b double, c string, d date, e timestamp')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bdf8931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Last Name: string (nullable = true)\n",
      " |-- First Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- State of Residence: string (nullable = true)\n",
      " |-- Education level: string (nullable = true)\n",
      "\n",
      "+---------+----------+---+------+------------------+---------------+\n",
      "|Last Name|First Name|Age|Gender|State of Residence|Education level|\n",
      "+---------+----------+---+------+------------------+---------------+\n",
      "|john     |john      |19 |male  |ny                |bachelor       |\n",
      "|amy      |amy       |23 |female|ny                |master         |\n",
      "|bob      |bob       |16 |male  |ca                |highSchool     |\n",
      "+---------+----------+---+------+------------------+---------------+\n",
      "\n",
      "+---------+----------+---+------+------------------+---------------+\n",
      "|Last Name|First Name|Age|Gender|State of Residence|Education level|\n",
      "+---------+----------+---+------+------------------+---------------+\n",
      "|      amy|       amy| 23|female|                ny|         master|\n",
      "+---------+----------+---+------+------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample Quiz Q1: read from text file\n",
    "rdd = sc.textFile('data.txt').map(lambda line: tuple(line.split()))\n",
    "cols = ['Last Name', 'First Name', 'Age', 'Gender', 'State of Residence', 'Education level']\n",
    "df2 = rdd.toDF(cols)\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)\n",
    "\n",
    "df2.createOrReplaceTempView(\"data\")\n",
    "query = '''\n",
    "SELECT * FROM data\n",
    "WHERE `State of Residence` = 'ny' AND Age >= 21\n",
    "'''\n",
    "sqlDF = spark.sql(query)\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd258aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c.net has rank: 0.7464008076748408.\n",
      "d.co has rank: 0.7844406865236148.\n",
      "a.com has rank: 0.838990182786046.\n",
      "b.com has rank: 0.4001096809515733.\n",
      "e.co has rank: 0.4001096809515733.\n",
      "bit.ly has rank: 0.8105766856306423.\n"
     ]
    }
   ],
   "source": [
    "# Sample Quiz Q2: PageRank\n",
    "import re\n",
    "from typing import Iterable, Tuple\n",
    "\n",
    "\n",
    "def computeContribs(urls: Iterable[str], rank: float) -> Iterable[Tuple[str, float]]:\n",
    "    \"\"\"Calculates URL contributions to the rank of other URLs.\"\"\"\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url, rank / num_urls)\n",
    "\n",
    "def parseNeighbors(urls: str) -> Tuple[str, str]:\n",
    "    \"\"\"Parses a urls pair string into urls pair.\"\"\"\n",
    "    parts = re.split(r'\\s+', urls)\n",
    "    return parts[0], parts[1]\n",
    "\n",
    "iterations = 10\n",
    "\n",
    "lines = spark.read.text('link.txt').rdd.map(lambda r: r[0])\n",
    "links = lines.map(lambda urls: parseNeighbors(urls)).distinct().groupByKey().cache()\n",
    "ranks = links.map(lambda url_neighbors: (url_neighbors[0], 1.0))\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    # Calculates URL contributions to the rank of other URLs.\n",
    "    contribs = links.join(ranks).flatMap(lambda url_urls_rank: computeContribs(\n",
    "        url_urls_rank[1][0], url_urls_rank[1][1]  # type: ignore[arg-type]\n",
    "    ))\n",
    "    # Re-calculates URL ranks based on neighbor contributions.\n",
    "    ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15)\n",
    "\n",
    "# Collects all URL ranks and dump them to console.\n",
    "for (link, rank) in ranks.collect():\n",
    "    print(\"%s has rank: %s.\" % (link, rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4740daf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/01 16:52:18 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to localhost:9009 - java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)\n",
      "\tat org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint.$anonfun$startReceiver$1(ReceiverTracker.scala:596)\n",
      "\tat org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint.$anonfun$startReceiver$1$adapted(ReceiverTracker.scala:586)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$submitJob$1(SparkContext.scala:2363)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "[Stage 29:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2022-05-01 16:52:20 -----------\n",
      "Error receiving from the stream!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jl/miniconda3/envs/6998/lib/python3.8/site-packages/pyspark/sql/context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "22/05/01 16:52:20 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to localhost:9009 - java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "[Stage 29:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2022-05-01 16:52:22 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error receiving from the stream!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/01 16:52:22 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to localhost:9009 - java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jl/miniconda3/envs/6998/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/jl/miniconda3/envs/6998/lib/python3.8/site-packages/py4j/clientserver.py\", line 475, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/home/jl/miniconda3/envs/6998/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "Exception in thread \"receiver-supervisor-future-2\" java.lang.InterruptedException: sleep interrupted\n",
      "\tat java.base/java.lang.Thread.sleep(Native Method)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:196)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "# Sample Quiz Q3: Spark Streams\n",
    "# Reference: https://www.toptal.com/apache/apache-spark-streaming-twitter\n",
    "'''\n",
    "# Code of twitter stream (producer, send side)\n",
    "import socket\n",
    "import sys\n",
    "import requests\n",
    "import requests_oauthlib\n",
    "import json\n",
    "\n",
    "ACCESS_TOKEN = 'YOUR_ACCESS_TOKEN'\n",
    "ACCESS_SECRET = 'YOUR_ACCESS_SECRET'\n",
    "CONSUMER_KEY = 'YOUR_CONSUMER_KEY'\n",
    "CONSUMER_SECRET = 'YOUR_CONSUMER_SECRET'\n",
    "my_auth = requests_oauthlib.OAuth1(CONSUMER_KEY, CONSUMER_SECRET,ACCESS_TOKEN, ACCESS_SECRET)\n",
    "\n",
    "def get_tweets():\n",
    "    url = 'https://stream.twitter.com/1.1/statuses/filter.json'\n",
    "    query_data = [('language', 'en'), ('locations', '-130,-20,100,50'),('track','#')]\n",
    "    query_url = url + '?' + '&'.join([str(t[0]) + '=' + str(t[1]) for t in query_data])\n",
    "    response = requests.get(query_url, auth=my_auth, stream=True)\n",
    "    print(query_url, response)\n",
    "    return response\n",
    "\n",
    "def send_tweets_to_spark(http_resp, tcp_connection):\n",
    "    for line in http_resp.iter_lines():\n",
    "        try:\n",
    "            full_tweet = json.loads(line)\n",
    "            tweet_text = full_tweet['text']\n",
    "            print(\"Tweet Text: \" + tweet_text)\n",
    "            print (\"------------------------------------------\")\n",
    "            tcp_connection.send(tweet_text + '\\n')\n",
    "        except:\n",
    "            e = sys.exc_info()[0]\n",
    "            print(\"Error: %s\" % e)\n",
    "            \n",
    "TCP_IP = \"localhost\"\n",
    "TCP_PORT = 9009\n",
    "conn = None\n",
    "s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "s.bind((TCP_IP, TCP_PORT))\n",
    "s.listen(1)\n",
    "print(\"Waiting for TCP connection...\")\n",
    "conn, addr = s.accept()\n",
    "print(\"Connected... Starting getting tweets.\")\n",
    "resp = get_tweets()\n",
    "send_tweets_to_spark(resp, conn)\n",
    "'''\n",
    "\n",
    "'''Receive side, consumer'''\n",
    "hashtag_of_interest = '#cloudcomputing'\n",
    "\n",
    "ssc = StreamingContext(sc, 2)\n",
    "ssc.checkpoint(\"TwitterStreamingProcCkpt\")\n",
    "dataStream = ssc.socketTextStream(\"localhost\", 9009)\n",
    "\n",
    "# split each tweet into words\n",
    "words = dataStream.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# filter the words to get only hashtags, then map each hashtag to be a pair of (hashtag,1)\n",
    "hashtags = words.filter(lambda w: hashtag_of_interest in w).map(lambda x: (x, 1))\n",
    "\n",
    "# adding the count of each hashtag to its last count\n",
    "def aggregate_tags_count(new_values, total_sum):\n",
    "    return sum(new_values) + (total_sum or 0)\n",
    "\n",
    "tags_totals = hashtags.updateStateByKey(aggregate_tags_count)\n",
    "\n",
    "# do processing for each RDD generated in each interval\n",
    "def get_sql_context_instance(spark_context):\n",
    "    if ('sqlContextSingletonInstance' not in globals()):\n",
    "        globals()['sqlContextSingletonInstance'] = SQLContext(spark_context)\n",
    "    return globals()['sqlContextSingletonInstance']\n",
    "\n",
    "def process_rdd(time, rdd):\n",
    "    print(\"----------- %s -----------\" % str(time))\n",
    "    try:\n",
    "        sql_context = get_sql_context_instance(rdd.context)\n",
    "\n",
    "        # convert the RDD to Row RDD\n",
    "        row_rdd = rdd.map(lambda w: Row(hashtag=w[0], hashtag_count=w[1]))\n",
    "\n",
    "        # create a DF from the Row RDD\n",
    "        hashtags_df = sql_context.createDataFrame(row_rdd)\n",
    "\n",
    "        # Register the dataframe as table\n",
    "        hashtags_df.registerTempTable(\"hashtags\")\n",
    "\n",
    "        # get the top 10 hashtags from the table using SQL and print them\n",
    "        hashtag_counts_df = sql_context.sql(\"select hashtag, hashtag_count from hashtags order by hashtag_count desc limit 10\")\n",
    "        hashtag_counts_df.show()\n",
    "    except:\n",
    "        print(\"Error receiving from the stream!\")\n",
    "\n",
    "tags_totals.foreachRDD(process_rdd)\n",
    "\n",
    "# start the streaming computation\n",
    "ssc.start()\n",
    "\n",
    "# wait for the streaming to finish\n",
    "try:\n",
    "    ssc.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79a05ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70f915f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
